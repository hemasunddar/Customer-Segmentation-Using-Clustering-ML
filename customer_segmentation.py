# -*- coding: utf-8 -*-
"""Customer Segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FvfeK7Cx1urhbpoVj_BnDy8hugVxrRf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.cluster.hierarchy as sch

from sklearn.preprocessing import LabelEncoder, StandardScaler
from matplotlib.patches import Ellipse
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.metrics import adjusted_rand_score, confusion_matrix

#Step 1: Load Dataset & Data Preprocessing
# ----------------------------
df = pd.read_csv("/content/Test.csv")
print("First 5 rows of dataset:")
print(df.head())

# 1.2 Data Preprocessing
# Checking the missing values
print("\nMissing values before handling:")
print(df.isnull().sum())

# Fill missing categorical values with mode
for col in ["Profession", "Graduated", "Ever_Married"]:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Fill missing numerical values with median
for col in ["Work_Experience", "Family_Size"]:
    df[col].fillna(df[col].median(), inplace=True)

print("\nMissing values after handling:")
print(df.isnull().sum())

# another method of fitting the null values and na
fill_values = {
    "Profession": df["Profession"].mode()[0],
    "Graduated": df["Graduated"].mode()[0],
    "Ever_Married": df["Ever_Married"].mode()[0],
    "Work_Experience": df["Work_Experience"].median(),
    "Family_Size": df["Family_Size"].median()
}

df = df.fillna(fill_values)

print("\nMissing values after handling:")
print(df.isnull().sum())

#1.3 Encode categorical variables
encoder = LabelEncoder()
df["Gender"] = encoder.fit_transform(df["Gender"])
df["Ever_Married"] = encoder.fit_transform(df["Ever_Married"])
df["Graduated"] = encoder.fit_transform(df["Graduated"])
df["Profession"] = encoder.fit_transform(df["Profession"])
df["Spending_Score"] = df["Spending_Score"].map({"Low": 0, "Average": 1, "High": 2})
df["Var_1"] = encoder.fit_transform(df["Var_1"])

# Select features for clustering
features = ["Age", "Work_Experience", "Family_Size",
            "Gender", "Ever_Married", "Graduated",
            "Profession", "Spending_Score"]
X = df[features]

# Show first 5 rows after encoding
print("\nData after encoding categorical variables:")
print(df.head())

# Show the selected features
print("\nSelected features for clustering:")
print(X.head())

# Show data types
print("\nData types of features:")
print(X.dtypes)

# NaN/Inf checking and scaling
print("\nAny NaN values left?", df.isnull().sum().sum())
print("Any infinite values?", np.isinf(df).sum().sum())

# Replace infinities if any
df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN safely (if any left)
df.fillna(0, inplace=True)

# Scale data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])

#1.4 Normalize numerical variables
# Separate numeric and categorical features
num_features = ["Age", "Work_Experience", "Family_Size"]
cat_features = ["Gender", "Ever_Married", "Graduated", "Profession", "Spending_Score"]

# 1. Scale only numeric features
scaler = StandardScaler()
X_num_scaled = scaler.fit_transform(df[num_features])

# 2. Keep categorical features as they are
X_cat = df[cat_features].values

# 3. Combine back into one dataset
X_scaled = np.hstack((X_num_scaled, X_cat))

# (Optional) Create a DataFrame for better readability
scaled_df = pd.DataFrame(
    X_scaled,
    columns=num_features + cat_features
)

print("\nScaled dataset preview:")
print(scaled_df.head())

# Step 2: Exploratory Data Analysis
# 2.1. Age distribution
plt.figure(figsize=(6,4))
sns.histplot(df["Age"], bins=20, kde=True, color="skyblue")
plt.title("Age Distribution of Customers")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

# 2.2. Gender split
plt.figure(figsize=(5,4))
sns.countplot(x="Gender", data=df, palette="Set2")
plt.title("Gender Distribution")
plt.xlabel("Gender (0=Female, 1=Male)")  # since encoded
plt.ylabel("Count")
plt.show()

# 2.3. Marital status split
plt.figure(figsize=(5,4))
sns.countplot(x="Ever_Married", data=df, palette="Set1")
plt.title("Marital Status Distribution")
plt.xlabel("Ever Married (0=No, 1=Yes)")
plt.ylabel("Count")
plt.show()

# 2.4. Profession-wise customer distribution
plt.figure(figsize=(8,5))
sns.countplot(x="Profession", data=df, palette="Paired")
plt.title("Profession-wise Customer Distribution")
plt.xlabel("Profession (encoded)")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

# 2.5. Spending Score Distribution
plt.figure(figsize=(6,4))
sns.countplot(x="Spending_Score", data=df)
plt.title("Spending Score Distribution")
plt.show()

# 2.6. Spending_Score vs Age
plt.figure(figsize=(7,5))
sns.boxplot(x="Spending_Score", y="Age", data=df, palette="coolwarm")
plt.title("Spending Score vs Age")
plt.xlabel("Spending Score (0=Low, 1=Average, 2=High)")
plt.ylabel("Age")
plt.show()

# 2.7. Spending_Score vs Family Size
plt.figure(figsize=(7,5))
sns.boxplot(x="Spending_Score", y="Family_Size", data=df, palette="viridis")
plt.title("Spending Score vs Family Size")
plt.xlabel("Spending Score (0=Low, 1=Average, 2=High)")
plt.ylabel("Family Size")
plt.show()

# 2.8. Category distribution in Var_1 (Cat_1 … Cat_6)
plt.figure(figsize=(6,4))
sns.countplot(x="Var_1", data=df, palette="Set3")
plt.title("Distribution of Var_1 Categories (Cat_1 to Cat_6)")
plt.xlabel("Var_1 (encoded)")
plt.ylabel("Count")
plt.show()

# Step 3: Feature Selection of Demographic, Professional, Behavioral
selected_features = [
    "Age", "Gender", "Family_Size", "Ever_Married",   # Demographic
    "Profession", "Work_Experience", "Graduated",     # Professional
    "Spending_Score"                                 # Behavioral
]
# Creating feature dataset
X = df[selected_features]

print("\nSelected Features for Clustering:")
print(X.head())

# Step 4: Clustering Techniques
# 4.1. K-Means Clustering
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)
df["KMeans_Cluster"] = kmeans_labels

# 4.2. Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=4)
hier_labels = hierarchical.fit_predict(X_scaled)
df["Hierarchical_Cluster"] = hier_labels

# 4.3. DBSCAN
dbscan = DBSCAN(eps=1.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)
df["DBSCAN_Cluster"] = dbscan_labels  # -1 means "noise"

# 4.4. Gaussian Mixture Models (GMM)
gmm = GaussianMixture(n_components=4, random_state=42)
gmm_labels = gmm.fit_predict(X_scaled)
df["GMM_Cluster"] = gmm_labels

print("\n✅ Clustering Done! Added 4 cluster labels to DataFrame.")
print(df[["KMeans_Cluster", "Hierarchical_Cluster", "DBSCAN_Cluster", "GMM_Cluster"]].head())

# Hierarchical Clustering Dendrogram visualization
plt.figure(figsize=(10, 5))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Customers")
plt.ylabel("Euclidean Distance")
plt.show()

# K-Means Visualization
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='rainbow', s=50)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], color='black', marker='x', s=200)
plt.title("K-Means Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# DBSCAN Visualization
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap='rainbow', s=50)
plt.title("DBSCAN Clustering")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Step 5: Cluster Evaluation
print("\n Cluster Evaluation Metrics:")

# Evaluate clustering
def evaluate_clustering(X, labels, method_name):
    # Some algorithms (like DBSCAN) can label noise points as -1; handle this case
    if len(set(labels)) <= 1 or (set(labels) == {-1}):
        print(f"{method_name}: Not enough clusters to evaluate.")
        return None

    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)
    calinski_harabasz = calinski_harabasz_score(X, labels)

    print(f"\n{method_name} Clustering:")
    print(f"Silhouette Score         : {silhouette:.4f} (higher is better)")
    print(f"Davies–Bouldin Index     : {davies_bouldin:.4f} (lower is better)")
    print(f"Calinski-Harabasz Index  : {calinski_harabasz:.4f} (higher is better)")

# Evaluate each method
evaluate_clustering(X_scaled, kmeans_labels, "K-Means")
evaluate_clustering(X_scaled, hier_labels, "Hierarchical")
evaluate_clustering(X_scaled, dbscan_labels, "DBSCAN")
evaluate_clustering(X_scaled, gmm_labels, "GMM")

# 5.2. Compare clusters with Var_1 to see if segmentation aligns with predefined categories.
def compare_clusters(labels, method_name):
    print(f"\n {method_name} vs Var_1:")

    # Adjusted Rand Index — measures similarity between clusters and true labels
    ari = adjusted_rand_score(df["Var_1"], labels)
    print(f"Adjusted Rand Index: {ari:.3f} (closer to 1 means better alignment)")

    # Confusion matrix
    cm = confusion_matrix(df["Var_1"], labels)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(f"{method_name} Confusion Matrix vs Var_1")
    plt.xlabel("Cluster Label")
    plt.ylabel("Var_1 Label")
    plt.show()

# Compare each clustering method
compare_clusters(kmeans_labels, "KMeans")
compare_clusters(hier_labels, "Hierarchical")
compare_clusters(dbscan_labels, "DBSCAN")
compare_clusters(gmm_labels, "Gaussian Mixture Model")

#Step 6: Results & Insights
#6.1 Identify Distinct Customer Groups
# Group by cluster using KMeans
cluster_profile = df.groupby("KMeans_Cluster").mean()

print("\n Cluster Profiles (KMeans):")
print(cluster_profile)

# Ex plaination of above output
#From this, you can label clusters like:
#Cluster 0: Young Professionals with Low Spending
#Cluster 1: Large Families with Average Spending
#Cluster 2: Middle-aged Adults with Medium Spending
#Cluster 3: Experienced Executives with High Spending

#6.2 Business Strategies for Each Group
# Example Code to Automate This Insight Extraction
def cluster_insights(df, cluster_col):
    print(f"\n Insights for {cluster_col}:")
    profile = df.groupby(cluster_col).mean()
    print(profile)

    for cluster in profile.index:
        print(f"\nCluster {cluster}:")
        print(f"Average Profile: {profile.loc[cluster].to_dict()}")

cluster_insights(df, "KMeans_Cluster")

# Step 6: Results & Insights
print("\n Cluster Profiles (KMeans):")
kmeans_profiles = df.groupby("KMeans_Cluster")[features].mean().round(2)
print(kmeans_profiles)

# Plot cluster profiles
df_profiles_melted = kmeans_profiles.reset_index().melt(id_vars=["KMeans_Cluster"])
plt.figure(figsize=(12, 8))
sns.barplot(data=df_profiles_melted, x="variable", y="value", hue="KMeans_Cluster")
plt.title("KMeans Cluster Profiles")
plt.ylabel("Average Value")
plt.xlabel("Feature")
plt.legend(title="Cluster")
plt.show()

# Business strategies
cluster_names = {
    0: "Young Low Spenders",
    1: "Experienced High Spenders",
    2: "Middle-aged Average Spenders",
    3: "Young Professionals"
}

strategies = {
    "Young Low Spenders": "Budget offers, loyalty rewards, student discounts",
    "Experienced High Spenders": "Premium offers, exclusive events, loyalty programs",
    "Middle-aged Average Spenders": "Family packages, seasonal promotions",
    "Young Professionals": "Targeted social media campaigns, starter bundles"
}

print("\n Cluster Insights & Strategies:")
for cluster_id, name in cluster_names.items():
    if cluster_id in kmeans_profiles.index:
        print(f"\nCluster {cluster_id} — {name}")
        print(f"Average Profile:\n{kmeans_profiles.loc[cluster_id]}")
        print(f"Strategy: {strategies[name]}")